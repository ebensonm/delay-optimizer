{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delay_optimizer import DelayedOptimizer\n",
    "from delay_optimizer.optimization.functions import Ackley, Rastrigin, Rosenbrock, Zakharov\n",
    "from delay_optimizer.optimization.optimizers import GradientDescent, Momentum, Adam\n",
    "from delay_optimizer.delays.distributions import Undelayed, Uniform, Stochastic\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 2,\n",
       " 'domain': (-32.768, 32.768),\n",
       " 'minimizer': array([0., 0.]),\n",
       " 'a': 20.0,\n",
       " 'b': 0.2,\n",
       " 'c': 6.283185307179586}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ackley(2).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 10,\n",
       " 'domain': (-5.12, 5.12),\n",
       " 'minimizer': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rastrigin(10).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 100,\n",
       " 'domain': (-5.0, 10.0),\n",
       " 'minimizer': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a': 1.0,\n",
       " 'b': 100.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rosenbrock(100).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 1000,\n",
       " 'domain': (-5.0, 10.0),\n",
       " 'minimizer': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zakharov(1000).__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(100,10000)\n",
    "dims = [2, 10, 100, 1000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'delay_optimizer.functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdelay_optimizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mold_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     ackley_gen,\n\u001b[1;32m      4\u001b[0m     rastrigin_gen,\n\u001b[1;32m      5\u001b[0m     rosenbrock_gen,\n\u001b[1;32m      6\u001b[0m     zakharov_gen\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_old_loss_function\u001b[39m(func):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mmatch\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(func)\u001b[38;5;241m.\u001b[39mlower():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'delay_optimizer.functions'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from delay_optimizer.functions.old_functions import (\n",
    "    ackley_gen,\n",
    "    rastrigin_gen,\n",
    "    rosenbrock_gen,\n",
    "    zakharov_gen\n",
    ")\n",
    "\n",
    "def get_old_loss_function(func):\n",
    "    match str(func).lower():\n",
    "        case 'ackley':\n",
    "            return ackley_gen(func.n)\n",
    "        case 'rastrigin':\n",
    "            return rastrigin_gen(func.n)\n",
    "        case 'rosenbrock':\n",
    "            return rosenbrock_gen(func.n)\n",
    "        case 'zakharov':\n",
    "            return zakharov_gen(func.n)\n",
    "\n",
    "def test_loss(func, x, n):\n",
    "    f = func(n)\n",
    "    f_old = get_old_loss_function(f)\n",
    "    x = x[:,:n]\n",
    "\n",
    "    t0 = time.time()\n",
    "    f1 = f.loss(x)\n",
    "    t1 = time.time()\n",
    "    f2 = [f.loss(x[i]) for i in range(x.shape[0])]\n",
    "    t2 = time.time()\n",
    "    f3 = [f_old(x[i]) for i in range(x.shape[0])]\n",
    "    t3 = time.time()\n",
    "\n",
    "    assert np.allclose(f1, f3)\n",
    "    assert np.allclose(f1, f2)\n",
    "    assert np.allclose(f2, f3)\n",
    "    return t1-t0, t2-t1, t3-t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001857280731201172 0.0018610954284667969 0.0018894672393798828\n",
      "0.0009121894836425781 0.0018253326416015625 0.010658979415893555\n",
      "0.0004627704620361328 0.002258777618408203 0.002112150192260742\n",
      "0.005427122116088867 0.007301807403564453 0.004945993423461914\n",
      "0.038039445877075195 0.027659177780151367 0.026062488555908203\n"
     ]
    }
   ],
   "source": [
    "# Ackley\n",
    "for d in dims:\n",
    "    print(*test_loss(Ackley, X, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00011301040649414062 0.0010266304016113281 0.0009617805480957031\n",
      "0.0006196498870849609 0.0011293888092041016 0.0010020732879638672\n",
      "0.0017309188842773438 0.0014655590057373047 0.0014944076538085938\n",
      "0.003254413604736328 0.0034799575805664062 0.0033440589904785156\n",
      "0.025737762451171875 0.021487712860107422 0.018725872039794922\n"
     ]
    }
   ],
   "source": [
    "# Rastrigin\n",
    "for d in dims:\n",
    "    print(*test_loss(Rastrigin, X, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.128715515136719e-05 0.0008788108825683594 0.0011162757873535156\n",
      "0.0002727508544921875 0.0009210109710693359 0.0011589527130126953\n",
      "0.0011010169982910156 0.0010349750518798828 0.0012493133544921875\n",
      "0.0015153884887695312 0.0016238689422607422 0.0019321441650390625\n",
      "0.011744976043701172 0.004252910614013672 0.0045812129974365234\n"
     ]
    }
   ],
   "source": [
    "# Rosenbrock\n",
    "for d in dims:\n",
    "    print(*test_loss(Rosenbrock, X, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00015473365783691406 0.0061414241790771484 0.005188941955566406\n",
      "0.0002658367156982422 0.0066127777099609375 0.0019745826721191406\n",
      "0.0012171268463134766 0.002131938934326172 0.0014562606811523438\n",
      "0.0003635883331298828 0.0016679763793945312 0.0017631053924560547\n",
      "0.004600048065185547 0.003712892532348633 0.005046367645263672\n"
     ]
    }
   ],
   "source": [
    "# Zakharov\n",
    "for d in dims:\n",
    "    print(*test_loss(Zakharov, X, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(100,10000)\n",
    "dims = [2, 10, 100, 1000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from delay_optimizer.functions.old_functions import (\n",
    "    ackley_deriv_gen,\n",
    "    rast_deriv_gen,\n",
    "    rosen_deriv_gen,\n",
    "    zakharov_deriv_gen\n",
    ")\n",
    "\n",
    "def get_old_grad_function(func):\n",
    "    match str(func).lower():\n",
    "        case 'ackley':\n",
    "            return ackley_deriv_gen(func.n)\n",
    "        case 'rastrigin':\n",
    "            return rast_deriv_gen(func.n)\n",
    "        case 'rosenbrock':\n",
    "            return rosen_deriv_gen(func.n)\n",
    "        case 'zakharov':\n",
    "            return zakharov_deriv_gen(func.n)\n",
    "\n",
    "def test_grad(func, x, n):\n",
    "    f = func(n)\n",
    "    f_old = get_old_grad_function(f)\n",
    "    x = x[:,:n]\n",
    "\n",
    "    t0 = time.time()\n",
    "    f1 = f.grad(x)\n",
    "    t1 = time.time()\n",
    "    f2 = [f.grad(x[i]) for i in range(x.shape[0])]\n",
    "    t2 = time.time()\n",
    "    f3 = [f_old(x[i]) for i in range(x.shape[0])]\n",
    "    t3 = time.time()\n",
    "\n",
    "    assert np.allclose(f1, f3)\n",
    "    assert np.allclose(f1, f2)\n",
    "    assert np.allclose(f2, f3)\n",
    "    return t1-t0, t2-t1, t3-t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00018334388732910156 0.003925800323486328 0.003774881362915039\n",
      "0.004918575286865234 0.008687734603881836 0.006947040557861328\n",
      "0.0012769699096679688 0.0055997371673583984 0.006273508071899414\n",
      "0.0071866512298583984 0.009026288986206055 0.008199453353881836\n",
      "0.050015926361083984 0.04297304153442383 0.041222333908081055\n"
     ]
    }
   ],
   "source": [
    "# Ackley\n",
    "for d in dims:\n",
    "    print(*test_grad(Ackley, X, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.510185241699219e-05 0.0005922317504882812 0.0004990100860595703\n",
      "0.00030732154846191406 0.0006930828094482422 0.0005869865417480469\n",
      "0.0003857612609863281 0.0015645027160644531 0.0008997917175292969\n",
      "0.0025892257690429688 0.0034923553466796875 0.0025403499603271484\n",
      "0.02410435676574707 0.02041769027709961 0.019176483154296875\n"
     ]
    }
   ],
   "source": [
    "# Rastrigin\n",
    "for d in dims:\n",
    "    print(*test_grad(Rastrigin, X, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001246929168701172 0.005778789520263672 0.006569862365722656\n",
      "0.0005772113800048828 0.0049724578857421875 0.005286216735839844\n",
      "0.00026535987854003906 0.0017685890197753906 0.0017848014831542969\n",
      "0.0024566650390625 0.0026204586029052734 0.0026044845581054688\n",
      "0.01853799819946289 0.009824514389038086 0.00918269157409668\n"
     ]
    }
   ],
   "source": [
    "# Rosenbrock\n",
    "for d in dims:\n",
    "    print(*test_grad(Rosenbrock, X, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001373291015625 0.005235910415649414 0.009942054748535156\n",
      "0.00042366981506347656 0.0015664100646972656 0.0012929439544677734\n",
      "0.004384756088256836 0.0016946792602539062 0.0013718605041503906\n",
      "0.0006747245788574219 0.002216339111328125 0.004332304000854492\n",
      "0.013399362564086914 0.00696110725402832 0.015995025634765625\n"
     ]
    }
   ],
   "source": [
    "# Zakharov\n",
    "for d in dims:\n",
    "    print(*test_grad(Zakharov, X, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the new Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delay_optimizer.optimization import schedulers as lr_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = Ackley(10)\n",
    "X = np.random.uniform(*objective.domain, size=(25, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from delay_optimizer.optimization.old_optimizers import (\n",
    "    GradientDescent as GradientDescentOld,\n",
    "    Adam as AdamOld,\n",
    "    Momentum as MomentumOld,\n",
    ")\n",
    "\n",
    "def get_old_optimizer(optimizer):\n",
    "    match optimizer.__class__.__name__:\n",
    "        case 'GradientDescent':\n",
    "            return GradientDescentOld\n",
    "        case 'Adam':\n",
    "            return AdamOld\n",
    "        case 'Momentum':\n",
    "            return MomentumOld\n",
    "\n",
    "def test_optimizer(optimizer, objective, X, **kwargs):\n",
    "    opt = optimizer(**kwargs)\n",
    "    kwargs['learning_rate'] = kwargs['lr']\n",
    "    opt_old = get_old_optimizer(opt)(params=kwargs) # Doesn't account for epsilon param\n",
    "\n",
    "    for x in X:\n",
    "        opt.initialize(x)\n",
    "        opt_old.initialize(x)\n",
    "\n",
    "        for i in range(1000):    # Maxiter=1000\n",
    "            grad = objective.grad(x)\n",
    "\n",
    "            x1 = opt.step(x, grad)\n",
    "            x2 = opt_old(x, grad, i+1)\n",
    "            try:\n",
    "                assert np.allclose(x1, x2)\n",
    "            except:\n",
    "                print(\"\")\n",
    "                print(x1)\n",
    "                print(x2)\n",
    "                print(i)\n",
    "                raise\n",
    "            x = x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optimizer(GradientDescent, objective, X, lr=lr_gen.constant(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optimizer(Adam, objective, X, lr=lr_gen.constant(0.01), beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optimizer(Momentum, objective, X, lr=lr_gen.constant(0.01), gamma=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the new Delayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delay_optimizer.generators import learning_rate_generator as lr_gen\n",
    "from delay_optimizer.generators.delay_distributions import Cyclical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = Rastrigin(2)\n",
    "optimizer = Adam\n",
    "maxiter = 1000\n",
    "delay_type = Cyclical([np.array([0,0]), np.array([1,0]), np.array([0,1]), np.array([1,1])], maxiter)\n",
    "X = np.random.uniform(*objective.domain, size=(100, 2))\n",
    "\n",
    "params = {'lr': lr_gen.constant(0.01), 'beta_1': 0.9, 'beta_2': 0.999}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delay_optimizer.optimizers.old_Delayer import Delayer as DelayerOld\n",
    "\n",
    "def test_delayer(objective, optimizer, delay_type, X, maxiter=1000, **kwargs):\n",
    "    optimizer = optimizer(**kwargs)\n",
    "    kwargs['learning_rate'] = kwargs['lr']\n",
    "    optimizer_old = get_old_optimizer(optimizer)(kwargs)\n",
    "    delayer = Delayer(objective, optimizer, delay_type)\n",
    "    delayer_old = DelayerOld(delay_type, objective, optimizer_old)\n",
    "\n",
    "    for x in X:\n",
    "        delayer.initialize(x)\n",
    "        delayer_old.initialize(x)\n",
    "        D_gen = delay_type.D_gen(objective.n)\n",
    "        \n",
    "        for i in range(1, maxiter+1):\n",
    "            delayer.step()\n",
    "            x1 = delayer.time_series[0]\n",
    "            x2 = delayer_old.update(i, next(D_gen))\n",
    "            try:\n",
    "                assert np.allclose(x1, x2)\n",
    "            except:\n",
    "                print(\"\")\n",
    "                print(x1)\n",
    "                print(x2)\n",
    "                print(i)\n",
    "                raise\n",
    "            x = x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delayer(objective, optimizer, delay_type, X, maxiter=maxiter, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Parallel Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delay Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delay_optimizer.delays.distributions import (\n",
    "    Undelayed, \n",
    "    Uniform, \n",
    "    Stochastic, \n",
    "    Decaying, \n",
    "    Partial, \n",
    "    Cyclical,\n",
    "    Constant\n",
    ")\n",
    "\n",
    "def test_delay_type(delay_type):\n",
    "    for size in [2, (2,), (5,2), (3,1,2), (10,5,3,2)]:\n",
    "        D_gen = delay_type.D_gen(size)\n",
    "        check_size = size if not isinstance(size, int) else tuple([size])\n",
    "        for i in range(delay_type.num_delays+10):\n",
    "            assert next(D_gen).shape == check_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delay_type(Undelayed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delay_type(Uniform(max_L=1, num_delays=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delay_type(Stochastic(max_L=1, num_delays=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delay_type(Decaying(max_L=1, num_delays=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delay_type(Partial(max_L=1, num_delays=10, p=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [np.array([0,0]), np.array([1,0]), np.array([0,1]), np.array([1,1])]\n",
    "test_delay_type(Cyclical(D, num_delays=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.array([1,0])\n",
    "test_delay_type(Constant(D, num_delays=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Parallel Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from delay_optimizer.optimization.old_optimizers import (\n",
    "    GradientDescent as GradientDescentOld,\n",
    "    Adam as AdamOld,\n",
    "    Momentum as MomentumOld,\n",
    ")\n",
    "from delay_optimizer.optimization import schedulers as lr_gen\n",
    "\n",
    "def get_old_optimizer(optimizer):\n",
    "    match optimizer.__class__.__name__:\n",
    "        case 'GradientDescent':\n",
    "            return GradientDescentOld\n",
    "        case 'Adam':\n",
    "            return AdamOld\n",
    "        case 'Momentum':\n",
    "            return MomentumOld\n",
    "\n",
    "def test_parallel_optimizer(optimizer, objective, X, maxiter=5000, **kwargs):\n",
    "    opt = optimizer(**kwargs)\n",
    "    kwargs['learning_rate'] = kwargs['lr']\n",
    "    opt_old = get_old_optimizer(opt)(params=kwargs) # Doesn't account for epsilon param\n",
    "\n",
    "    def loop(optimizer, x_init, is_old=False):\n",
    "        x = x_init\n",
    "        for i in range(maxiter):\n",
    "            grad = objective.grad(x)\n",
    "            assert grad.shape == x.shape\n",
    "            if is_old:\n",
    "                x = optimizer(x, grad, i+1)\n",
    "            else:\n",
    "                x = optimizer.step(x, grad)\n",
    "        return x\n",
    "\n",
    "    start1 = time.time()\n",
    "    opt.initialize(X)\n",
    "    final_X = loop(opt, X, False)\n",
    "    print(\"Time taken for new optimizer:\", time.time()-start1)\n",
    "\n",
    "    final_X_old = []\n",
    "    start2 = time.time()\n",
    "    for x in X:\n",
    "        opt_old.initialize(x)\n",
    "        final_X_old.append(loop(opt_old, x, True))\n",
    "    print(\"Time taken for old optimizer:\", time.time()-start2)\n",
    "\n",
    "    try:\n",
    "        assert np.allclose(final_X, final_X_old)\n",
    "    except:\n",
    "        print(\"\")\n",
    "        mask = ~np.isclose(final_X, final_X_old)\n",
    "        print(final_X[mask])\n",
    "        print(np.array(final_X_old)[mask])\n",
    "        raise\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = Ackley(10)\n",
    "X = np.random.uniform(*objective.domain, size=(100, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for new optimizer: 0.33013296127319336\n",
      "Time taken for old optimizer: 13.63086748123169\n"
     ]
    }
   ],
   "source": [
    "test_parallel_optimizer(GradientDescent, objective, X, lr=lr_gen.constant(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for new optimizer: 0.4525609016418457\n",
      "Time taken for old optimizer: 22.806904077529907\n"
     ]
    }
   ],
   "source": [
    "test_parallel_optimizer(Adam, objective, X, lr=lr_gen.constant(0.01), beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for new optimizer: 0.36785125732421875\n",
      "Time taken for old optimizer: 14.358436346054077\n"
     ]
    }
   ],
   "source": [
    "test_parallel_optimizer(Momentum, objective, X, lr=lr_gen.constant(0.01), gamma=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Parallel Delayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delay_optimizer.delays.old_Delayer import Delayer as DelayerOld\n",
    "from delay_optimizer.optimization import schedulers as lr_gen\n",
    "from delay_optimizer.delays.distributions import Cyclical\n",
    "\n",
    "def test_parallel_delayer(objective, optimizer, delay_type, X, maxiter=1000, **kwargs):\n",
    "    optimizer = optimizer(**kwargs)\n",
    "    kwargs['learning_rate'] = kwargs['lr']\n",
    "    optimizer_old = get_old_optimizer(optimizer)(kwargs)\n",
    "    delayer = DelayedOptimizer(objective, optimizer, delay_type)\n",
    "    delayer_old = DelayerOld(delay_type, objective, optimizer_old, save_state=True)\n",
    "\n",
    "    start1 = time.time()\n",
    "    final_X = delayer.optimize(X, maxiter)\n",
    "    print(\"Time taken for new delayer:\", time.time()-start1)\n",
    "\n",
    "    start2 = time.time()\n",
    "    final_X_old = []\n",
    "    for x in X:\n",
    "        final_X_old.append(delayer_old.optimize(x, maxiter).state_vals[-1])\n",
    "    print(\"Time taken for old delayer:\", time.time()-start2)\n",
    "    \n",
    "    try:\n",
    "        assert np.allclose(final_X, final_X_old)\n",
    "    except:\n",
    "        print(\"\")\n",
    "        print(final_X)\n",
    "        print(final_X_old)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = Rastrigin(10)\n",
    "optimizer = Adam\n",
    "maxiter = 1000\n",
    "\n",
    "D = np.random.randint(0,4,(4,10))\n",
    "delay_type = Cyclical(D, num_delays=maxiter)\n",
    "X = np.random.uniform(*objective.domain, size=(100, 10))\n",
    "\n",
    "params = {'lr': lr_gen.constant(0.01), 'beta_1': 0.9, 'beta_2': 0.999}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for new delayer: 0.1709308624267578\n",
      "Time taken for old delayer: 10.97342324256897\n"
     ]
    }
   ],
   "source": [
    "test_parallel_delayer(objective, optimizer, delay_type, X, maxiter=maxiter, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Length Timed Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44316952,  0.71069238, -6.7132886 , ..., -4.54592862,\n",
       "        -0.50964139, -1.6709056 ],\n",
       "       [ 2.65895653, -4.58637929,  5.075438  , ..., -4.72365731,\n",
       "         7.28903772, -1.8125978 ],\n",
       "       [-3.58935223,  0.34003511, -4.29081049, ..., -3.26300843,\n",
       "         7.17691409, -3.16623606],\n",
       "       ...,\n",
       "       [ 7.34487282,  4.89893381, -1.86016198, ..., -0.27419088,\n",
       "         4.61734544,  6.80495955],\n",
       "       [ 1.83187627,  3.53682214, -1.79889252, ...,  1.85404465,\n",
       "        -2.88804457, -7.40060427],\n",
       "       [-6.86166156, -7.5168008 ,  3.43053027, ...,  3.48710383,\n",
       "         1.7711286 ,  4.15824122]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delay_optimizer import optimizers, functions, DelayedOptimizer\n",
    "from delay_optimizer.delays import distributions\n",
    "import numpy as np\n",
    "\n",
    "optimizer = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
    "objective = functions.Zakharov(1000)\n",
    "maxiter = 10000\n",
    "delay_type = distributions.Stochastic(max_L=1, num_delays=maxiter)\n",
    "X = np.random.uniform(*objective.domain, size=(1000, 1000))\n",
    "\n",
    "delayer = DelayedOptimizer(objective, optimizer, delay_type)\n",
    "delayer.optimize(X, maxiter=maxiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a time of around 7 mins, assuming this run would have been around 2-3 hours, we have around a 20x speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

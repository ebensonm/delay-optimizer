{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from delay_optimizer.functions import Ackley, Rastrigin, Rosenbrock, Zakharov\n",
    "from delay_optimizer.optimizers import GradientDescent, Adam, Momentum, NesterovMomentum\n",
    "from delay_optimizer.optimizers import Delayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 2,\n",
       " 'domain': (-32.768, 32.768),\n",
       " 'minimizer': array([0., 0.]),\n",
       " 'a': 20.0,\n",
       " 'b': 0.2,\n",
       " 'c': 6.283185307179586}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ackley(2).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 10,\n",
       " 'domain': (-5.12, 5.12),\n",
       " 'minimizer': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rastrigin(10).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 100,\n",
       " 'domain': (-5.0, 10.0),\n",
       " 'minimizer': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a': 1.0,\n",
       " 'b': 100.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rosenbrock(100).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 1000,\n",
       " 'domain': (-5.0, 10.0),\n",
       " 'minimizer': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zakharov(1000).__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(100,10000)\n",
    "dims = [2, 10, 100, 1000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from delay_optimizer.functions.old_functions import (\n",
    "    ackley_gen,\n",
    "    rastrigin_gen,\n",
    "    rosenbrock_gen,\n",
    "    zakharov_gen\n",
    ")\n",
    "\n",
    "def get_old_loss_function(func):\n",
    "    match str(func).lower():\n",
    "        case 'ackley':\n",
    "            return ackley_gen(func.n)\n",
    "        case 'rastrigin':\n",
    "            return rastrigin_gen(func.n)\n",
    "        case 'rosenbrock':\n",
    "            return rosenbrock_gen(func.n)\n",
    "        case 'zakharov':\n",
    "            return zakharov_gen(func.n)\n",
    "\n",
    "def test_loss(func, x, n):\n",
    "    f = func(n)\n",
    "    f_old = get_old_loss_function(f)\n",
    "    x = x[:,:n]\n",
    "\n",
    "    t0 = time.time()\n",
    "    f1 = f.loss(x)\n",
    "    t1 = time.time()\n",
    "    f2 = [f.loss(x[i]) for i in range(x.shape[0])]\n",
    "    t2 = time.time()\n",
    "    f3 = [f_old(x[i]) for i in range(x.shape[0])]\n",
    "    t3 = time.time()\n",
    "\n",
    "    assert np.allclose(f1, f3)\n",
    "    assert np.allclose(f1, f2)\n",
    "    assert np.allclose(f2, f3)\n",
    "    return t1-t0, t2-t1, t3-t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001857280731201172 0.0018610954284667969 0.0018894672393798828\n",
      "0.0009121894836425781 0.0018253326416015625 0.010658979415893555\n",
      "0.0004627704620361328 0.002258777618408203 0.002112150192260742\n",
      "0.005427122116088867 0.007301807403564453 0.004945993423461914\n",
      "0.038039445877075195 0.027659177780151367 0.026062488555908203\n"
     ]
    }
   ],
   "source": [
    "# Ackley\n",
    "for d in dims:\n",
    "    print(*test_loss(Ackley, X, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00011301040649414062 0.0010266304016113281 0.0009617805480957031\n",
      "0.0006196498870849609 0.0011293888092041016 0.0010020732879638672\n",
      "0.0017309188842773438 0.0014655590057373047 0.0014944076538085938\n",
      "0.003254413604736328 0.0034799575805664062 0.0033440589904785156\n",
      "0.025737762451171875 0.021487712860107422 0.018725872039794922\n"
     ]
    }
   ],
   "source": [
    "# Rastrigin\n",
    "for d in dims:\n",
    "    print(*test_loss(Rastrigin, X, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.128715515136719e-05 0.0008788108825683594 0.0011162757873535156\n",
      "0.0002727508544921875 0.0009210109710693359 0.0011589527130126953\n",
      "0.0011010169982910156 0.0010349750518798828 0.0012493133544921875\n",
      "0.0015153884887695312 0.0016238689422607422 0.0019321441650390625\n",
      "0.011744976043701172 0.004252910614013672 0.0045812129974365234\n"
     ]
    }
   ],
   "source": [
    "# Rosenbrock\n",
    "for d in dims:\n",
    "    print(*test_loss(Rosenbrock, X, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00015473365783691406 0.0061414241790771484 0.005188941955566406\n",
      "0.0002658367156982422 0.0066127777099609375 0.0019745826721191406\n",
      "0.0012171268463134766 0.002131938934326172 0.0014562606811523438\n",
      "0.0003635883331298828 0.0016679763793945312 0.0017631053924560547\n",
      "0.004600048065185547 0.003712892532348633 0.005046367645263672\n"
     ]
    }
   ],
   "source": [
    "# Zakharov\n",
    "for d in dims:\n",
    "    print(*test_loss(Zakharov, X, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(100,10000)\n",
    "dims = [2, 10, 100, 1000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from delay_optimizer.functions.old_functions import (\n",
    "    ackley_deriv_gen,\n",
    "    rast_deriv_gen,\n",
    "    rosen_deriv_gen,\n",
    "    zakharov_deriv_gen\n",
    ")\n",
    "\n",
    "def get_old_grad_function(func):\n",
    "    match str(func).lower():\n",
    "        case 'ackley':\n",
    "            return ackley_deriv_gen(func.n)\n",
    "        case 'rastrigin':\n",
    "            return rast_deriv_gen(func.n)\n",
    "        case 'rosenbrock':\n",
    "            return rosen_deriv_gen(func.n)\n",
    "        case 'zakharov':\n",
    "            return zakharov_deriv_gen(func.n)\n",
    "\n",
    "def test_grad(func, x, n):\n",
    "    f = func(n)\n",
    "    f_old = get_old_grad_function(f)\n",
    "    x = x[:,:n]\n",
    "\n",
    "    t0 = time.time()\n",
    "    f1 = f.grad(x)\n",
    "    t1 = time.time()\n",
    "    f2 = [f.grad(x[i]) for i in range(x.shape[0])]\n",
    "    t2 = time.time()\n",
    "    f3 = [f_old(x[i]) for i in range(x.shape[0])]\n",
    "    t3 = time.time()\n",
    "\n",
    "    assert np.allclose(f1, f3)\n",
    "    assert np.allclose(f1, f2)\n",
    "    assert np.allclose(f2, f3)\n",
    "    return t1-t0, t2-t1, t3-t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00018334388732910156 0.003925800323486328 0.003774881362915039\n",
      "0.004918575286865234 0.008687734603881836 0.006947040557861328\n",
      "0.0012769699096679688 0.0055997371673583984 0.006273508071899414\n",
      "0.0071866512298583984 0.009026288986206055 0.008199453353881836\n",
      "0.050015926361083984 0.04297304153442383 0.041222333908081055\n"
     ]
    }
   ],
   "source": [
    "# Ackley\n",
    "for d in dims:\n",
    "    print(*test_grad(Ackley, X, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.510185241699219e-05 0.0005922317504882812 0.0004990100860595703\n",
      "0.00030732154846191406 0.0006930828094482422 0.0005869865417480469\n",
      "0.0003857612609863281 0.0015645027160644531 0.0008997917175292969\n",
      "0.0025892257690429688 0.0034923553466796875 0.0025403499603271484\n",
      "0.02410435676574707 0.02041769027709961 0.019176483154296875\n"
     ]
    }
   ],
   "source": [
    "# Rastrigin\n",
    "for d in dims:\n",
    "    print(*test_grad(Rastrigin, X, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001246929168701172 0.005778789520263672 0.006569862365722656\n",
      "0.0005772113800048828 0.0049724578857421875 0.005286216735839844\n",
      "0.00026535987854003906 0.0017685890197753906 0.0017848014831542969\n",
      "0.0024566650390625 0.0026204586029052734 0.0026044845581054688\n",
      "0.01853799819946289 0.009824514389038086 0.00918269157409668\n"
     ]
    }
   ],
   "source": [
    "# Rosenbrock\n",
    "for d in dims:\n",
    "    print(*test_grad(Rosenbrock, X, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001373291015625 0.005235910415649414 0.009942054748535156\n",
      "0.00042366981506347656 0.0015664100646972656 0.0012929439544677734\n",
      "0.004384756088256836 0.0016946792602539062 0.0013718605041503906\n",
      "0.0006747245788574219 0.002216339111328125 0.004332304000854492\n",
      "0.013399362564086914 0.00696110725402832 0.015995025634765625\n"
     ]
    }
   ],
   "source": [
    "# Zakharov\n",
    "for d in dims:\n",
    "    print(*test_grad(Zakharov, X, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the new Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delay_optimizer.generators import learning_rate_generator as lr_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = Ackley(10)\n",
    "X = np.random.uniform(*objective.domain, size=(25, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from delay_optimizer.optimizers.old_optimizers import (\n",
    "    GradientDescent as GradientDescentOld,\n",
    "    Adam as AdamOld,\n",
    "    Momentum as MomentumOld,\n",
    "    NesterovMomentum as NesterovMomentumOld,\n",
    ")\n",
    "\n",
    "def get_old_optimizer(optimizer):\n",
    "    match optimizer.__class__.__name__:\n",
    "        case 'GradientDescent':\n",
    "            return GradientDescentOld\n",
    "        case 'Adam':\n",
    "            return AdamOld\n",
    "        case 'Momentum':\n",
    "            return MomentumOld\n",
    "        case 'NesterovMomentum':\n",
    "            return NesterovMomentumOld\n",
    "\n",
    "def test_optimizer(optimizer, objective, X, **kwargs):\n",
    "    opt = optimizer(**kwargs)\n",
    "    kwargs['learning_rate'] = kwargs['lr']\n",
    "    opt_old = get_old_optimizer(opt)(params=kwargs) # Doesn't account for epsilon param\n",
    "\n",
    "    for x in X:\n",
    "        opt.initialize(x)\n",
    "        opt_old.initialize(x)\n",
    "\n",
    "        for i in range(1000):    # Maxiter=100\n",
    "            grad = objective.grad(x)\n",
    "\n",
    "            x1 = opt.step(x, grad)\n",
    "            x2 = opt_old(x, grad, i+1)\n",
    "            try:\n",
    "                assert np.allclose(x1, x2)\n",
    "            except:\n",
    "                print(\"\")\n",
    "                print(x1)\n",
    "                print(x2)\n",
    "                print(i)\n",
    "                raise\n",
    "            x = x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optimizer(GradientDescent, objective, X, lr=lr_gen.constant(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optimizer(Adam, objective, X, lr=lr_gen.constant(0.01), beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optimizer(Momentum, objective, X, lr=lr_gen.constant(0.01), gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optimizer(NesterovMomentum, objective, X, lr=lr_gen.constant(0.01), gamma=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the new Delayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Delayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delay_optimizer.generators import learning_rate_generator as lr_gen\n",
    "from delay_optimizer.generators.delay_distributions import Cyclical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = Rastrigin(2)\n",
    "optimizer = Adam\n",
    "maxiter = 1000\n",
    "delay_type = Cyclical([np.array([0,0]), np.array([1,0]), np.array([0,1]), np.array([1,1])], maxiter)\n",
    "X = np.random.uniform(*objective.domain, size=(100, 2))\n",
    "\n",
    "params = {'lr': lr_gen.constant(0.01), 'beta_1': 0.9, 'beta_2': 0.999}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delay_optimizer.optimizers.old_Delayer import Delayer as DelayerOld\n",
    "\n",
    "def test_delayer(objective, optimizer, delay_type, X, maxiter=1000, **kwargs):\n",
    "    optimizer = optimizer(**kwargs)\n",
    "    kwargs['learning_rate'] = kwargs['lr']\n",
    "    optimizer_old = get_old_optimizer(optimizer)(kwargs)\n",
    "    delayer = Delayer(objective, optimizer, delay_type)\n",
    "    delayer_old = DelayerOld(delay_type, objective, optimizer_old)\n",
    "\n",
    "    for x in X:\n",
    "        delayer.initialize(x)\n",
    "        delayer_old.initialize(x)\n",
    "        D_gen = delay_type.D_gen(objective.n)\n",
    "        \n",
    "        for i in range(1, maxiter+1):\n",
    "            delayer.step()\n",
    "            x1 = delayer.time_series[0]\n",
    "            x2 = delayer_old.update(i, next(D_gen))\n",
    "            try:\n",
    "                assert np.allclose(x1, x2)\n",
    "            except:\n",
    "                print(\"\")\n",
    "                print(x1)\n",
    "                print(x2)\n",
    "                print(i)\n",
    "                raise\n",
    "            x = x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delayer(objective, optimizer, delay_type, X, maxiter=maxiter, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Simultaneous Delay Generation for multiple points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delay_optimizer.generators.delay_distributions import (\n",
    "    Undelayed, \n",
    "    Uniform, \n",
    "    Stochastic, \n",
    "    Decaying, \n",
    "    Partial, \n",
    "    Cyclical,\n",
    "    Constant\n",
    ")\n",
    "\n",
    "def test_delay_type(delay_type):\n",
    "    for size in [2, (2,), (5,2), (3,1,2), (10,5,3,2)]:\n",
    "        D_gen = delay_type.D_gen(size)\n",
    "        check_size = size if not isinstance(size, int) else tuple([size])\n",
    "        for i in range(delay_type.num_delays+10):\n",
    "            assert next(D_gen).shape == check_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delay_type(Undelayed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delay_type(Uniform(max_L=1, num_delays=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delay_type(Stochastic(max_L=1, num_delays=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delay_type(Decaying(max_L=1, num_delays=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delay_type(Partial(max_L=1, num_delays=10, p=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [np.array([0,0]), np.array([1,0]), np.array([0,1]), np.array([1,1])]\n",
    "test_delay_type(Cyclical(D, num_delays=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.array([1,0])\n",
    "test_delay_type(Constant(D, num_delays=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Parallel Delayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class Delayer:\n",
    "    \"\"\"Class that performs delayed optimization on the given objective function \n",
    "    with the given optimizer. The sequence of delays to use is determined by the \n",
    "    DelayType object.\n",
    "    \"\"\"\n",
    "    def __init__(self, objective, optimizer, delay_type):\n",
    "        \"\"\"Initializer for the Delayer class\n",
    "            \n",
    "        Parameters:\n",
    "            objective (ObjectiveFunction): the loss function to be optimized over, \n",
    "                including dimension n, loss function, and gradient function\n",
    "            optimizer (Optimizer): an initialized class object to perform the\n",
    "                optimization. Must have a step() method that updates the state\n",
    "                given state and gradient values.\n",
    "            delay_type (DelayType): object containing delay parameters\n",
    "        \"\"\"\n",
    "        self.objective = objective\n",
    "        self.optimizer = optimizer\n",
    "        self.delay_type = delay_type\n",
    "        \n",
    "    def initialize(self, x_init):\n",
    "        self.optimizer.initialize(x_init)\n",
    "        self.time_series = np.tile(x_init, (self.delay_type.max_L+1, 1))\n",
    "        self.delay_generator = self.delay_type.D_gen(self.objective.n)\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Computes the delayed state and gradient values and takes a step\n",
    "        with the optimizer.\n",
    "        \"\"\"\n",
    "        # Compute the delayed state and gradient\n",
    "        D = next(self.delay_generator)              # Get the delay\n",
    "        del_state = np.diag(self.time_series[D])\n",
    "        del_grad = self.objective.grad(del_state)\n",
    "        \n",
    "        # Roll the time series forward and update\n",
    "        self.time_series = np.roll(self.time_series, 1, axis=0)\n",
    "        self.time_series[0] = self.optimizer.step(del_state, del_grad) \n",
    "\n",
    "        return self.time_series[0]   \n",
    "        \n",
    "    def optimize(self, x_init, maxiter=5000):\n",
    "        \"\"\"Computes the time series using the passed optimizer from __init__, \n",
    "        saves convergence and time_series which is an array of the states\n",
    "           \n",
    "        Parameters:\n",
    "            x_init (ndarray): the initial state of the calculation\n",
    "            maxiter (int): the max number of iterations\n",
    "        Returns:\n",
    "            (Result): an object containing the optimization data\n",
    "        \"\"\"\n",
    "        self.initialize(x_init) \n",
    "        for i in range(maxiter):\n",
    "            x = self.step()\n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delay_optimizer.functions import Ackley\n",
    "from delay_optimizer.optimizers import Adam\n",
    "\n",
    "n = 10\n",
    "objective = Ackley(n)\n",
    "optimizer = Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
    "X = np.random.uniform(*objective.domain, size=(100, n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
